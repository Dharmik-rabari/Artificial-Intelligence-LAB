# -*- coding: utf-8 -*-
"""Week5(Portfolio).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1VQKbSuDU1Bg3dr0gxDN7TaBHWMOxiRxg
"""

import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

import tensorflow as tf
from tensorflow.keras import layers, models
from tensorflow.keras.datasets import cifar10
from tensorflow.keras.models import Model
from sklearn.metrics import confusion_matrix, classification_report

# Load CIFAR-10 dataset
(X_train, y_train), (X_test, y_test) = cifar10.load_data()

print("Train shape:", X_train.shape)
print("Test shape:", X_test.shape)

# Normalize pixel values to [0,1]
X_train = X_train.astype("float32") / 255.0
X_test = X_test.astype("float32") / 255.0

# Flatten labels to shape (n,)
y_train = y_train.reshape(-1)
y_test = y_test.reshape(-1)

# Class names for CIFAR-10
class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer',
               'dog', 'frog', 'horse', 'ship', 'truck']

def build_cnn_with_pooling():
    model = models.Sequential()
    model.add(layers.Conv2D(32, (3,3), activation='relu', input_shape=(32,32,3)))
    model.add(layers.MaxPooling2D((2,2)))

    model.add(layers.Conv2D(64, (3,3), activation='relu'))
    model.add(layers.MaxPooling2D((2,2)))

    model.add(layers.Conv2D(128, (3,3), activation='relu'))

    model.add(layers.Flatten())
    model.add(layers.Dense(128, activation='relu'))
    model.add(layers.Dense(10, activation='softmax'))

    model.compile(optimizer='adam',
                  loss='sparse_categorical_crossentropy',
                  metrics=['accuracy'])
    return model

model_pool = build_cnn_with_pooling()
model_pool.summary()

history_pool = model_pool.fit(
    X_train, y_train,
    epochs=5,
    batch_size=64,
    validation_split=0.1,
    verbose=1
)

test_loss_pool, test_acc_pool = model_pool.evaluate(X_test, y_test, verbose=0)
print("Test accuracy (with pooling):", test_acc_pool)

def build_cnn_without_pooling():
    model = models.Sequential()
    model.add(layers.Conv2D(32, (3,3), activation='relu', input_shape=(32,32,3)))
    model.add(layers.Conv2D(32, (3,3), activation='relu'))

    model.add(layers.Conv2D(64, (3,3), activation='relu'))
    model.add(layers.Conv2D(64, (3,3), activation='relu'))

    model.add(layers.Flatten())
    model.add(layers.Dense(128, activation='relu'))
    model.add(layers.Dense(10, activation='softmax'))

    model.compile(optimizer='adam',
                  loss='sparse_categorical_crossentropy',
                  metrics=['accuracy'])
    return model

model_nopool = build_cnn_without_pooling()
model_nopool.summary()

history_nopool = model_nopool.fit(
    X_train, y_train,
    epochs=5,
    batch_size=64,
    validation_split=0.1,
    verbose=1
)

test_loss_nopool, test_acc_nopool = model_nopool.evaluate(X_test, y_test, verbose=0)
print("Test accuracy (without pooling):", test_acc_nopool)

print("Test accuracy with pooling:   {:.4f}".format(test_acc_pool))
print("Test accuracy without pooling:{:.4f}".format(test_acc_nopool))

# Build a functional model similar to model_pool for Grad-CAM
inputs = layers.Input(shape=(32,32,3))
x = layers.Conv2D(32, (3,3), activation='relu')(inputs)
x = layers.MaxPooling2D((2,2))(x)
x = layers.Conv2D(64, (3,3), activation='relu')(x)
x = layers.MaxPooling2D((2,2))(x)
x = layers.Conv2D(128, (3,3), activation='relu', name='last_conv')(x)
x = layers.Flatten()(x)
x = layers.Dense(128, activation='relu')(x)
outputs = layers.Dense(10, activation='softmax')(x)

model_cam = Model(inputs, outputs)

model_cam.compile(optimizer='adam',
                  loss='sparse_categorical_crossentropy',
                  metrics=['accuracy'])

history_cam = model_cam.fit(
    X_train, y_train,
    epochs=5,
    batch_size=64,
    validation_split=0.1,
    verbose=1
)

test_loss_cam, test_acc_cam = model_cam.evaluate(X_test, y_test, verbose=0)
print("Test accuracy (Grad-CAM model):", test_acc_cam)

def compute_gradcam(model, image, class_index, layer_name='last_conv'):
    grad_model = Model(
        inputs=model.inputs,
        outputs=[model.get_layer(layer_name).output, model.output]
    )

    with tf.GradientTape() as tape:
        inputs = tf.cast(tf.expand_dims(image, axis=0), tf.float32)
        conv_outputs, predictions = grad_model(inputs)
        loss = predictions[:, class_index]

    grads = tape.gradient(loss, conv_outputs)
    pooled_grads = tf.reduce_mean(grads, axis=(0,1,2))
    conv_outputs = conv_outputs[0]

    heatmap = conv_outputs @ pooled_grads[..., tf.newaxis]
    heatmap = tf.squeeze(heatmap)
    heatmap = tf.maximum(heatmap, 0) / tf.math.reduce_max(heatmap)
    return heatmap.numpy()

indices = [0, 10, 100, 200]

for i in indices:
    image = X_test[i]
    label = y_test[i]

    # predicted class
    preds = model_cam.predict(image[np.newaxis, ...], verbose=0)
    pred_class = np.argmax(preds[0])

    target_class = pred_class

    heatmap = compute_gradcam(model_cam, image, target_class)

    # resize heatmap to 32x32
    heatmap_resized = tf.image.resize(heatmap[..., np.newaxis], [32,32]).numpy().squeeze()

    plt.figure(figsize=(4,2))
    plt.subplot(1,2,1)
    plt.imshow(image)
    plt.title(f"Label: {class_names[label]}\nPred: {class_names[pred_class]}")
    plt.axis('off')

    plt.subplot(1,2,2)
    plt.imshow(image)
    plt.imshow(heatmap_resized, cmap='jet', alpha=0.5)
    plt.title("Grad-CAM")
    plt.axis('off')

    plt.tight_layout()
    plt.show()

"""In this week's lab we worked with CIFAR-10 dataset. We trained neural networks so that they can classify between the images. The dataset we used, contain 10 object classes which are all different. We normalised the image before training, so that it can learn better. The first model that we used, has maxpooling layers and the second one did not used that. To make sure it works properly, We chekced test accuracies of the models after the training, and the results showd that the model with the pooling, has high accuracy as comapred to the other one that did not use pooling. This results tells us that pooling help getting good and accurate results which makes the training easier. We worked with Grad-CAM as well to check what parts of the image was focused by the model to make predictions, and it showed in the heatmap that important parts of the image that model sees. This lab helped learn about the effects of pooling on models performance and gainig visual explainations of models."""