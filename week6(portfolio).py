# -*- coding: utf-8 -*-
"""Week6(Portfolio).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1J7nfqa3soRDlz3WUiuE_sYGt9zk2gb7V
"""

import numpy as np
from sklearn.cluster import KMeans
from sklearn import metrics
import pandas as pd
df = pd.read_csv('diabetesN.csv')
df.head()

df.corr()

df.info()
X= df['Glucose']
Y=df['Outcome']
import numpy as np
data=np.array(list(zip(X,Y)))
data
X

# same thing using index location
xx = df.iloc[:,[3,5]]
xx

from yellowbrick.cluster import KElbowVisualizer
model = KMeans()
visualizer = KElbowVisualizer(model, k=(1,12)).fit(xx)
visualizer.show()

from sklearn.cluster import KMeans
kmeans = KMeans(n_clusters=3, init='k-means++', random_state=0).fit(xx)

kmeans.labels_

#We assign the labels to each row in dataframe.
df["Clus_km"] = kmeans.labels_
df.head(15)

kmeans.cluster_centers_

#Get each cluster size,
from collections import Counter
Counter(kmeans.labels_)

from sklearn import metrics
clusters = kmeans.labels_.tolist()

ClusterCentroid=kmeans.cluster_centers_
clustervalidation=metrics.silhouette_score(xx,clusters, metric='euclidean')
print(clustervalidation)

import seaborn as sns
import matplotlib.pyplot as plt
sns.scatterplot(data=xx, x="SkinThickness", y="BMI", hue=kmeans.labels_)
plt.show()

sns.scatterplot(data=xx, x="SkinThickness", y="BMI", hue=kmeans.labels_)
plt.scatter(kmeans.cluster_centers_[:,0], kmeans.cluster_centers_[:,1],
            marker="X", c="r", s=80, label="centroids")
plt.legend()
plt.show()

newdata=[[85.89,30400],[10,10000],[15,1700],[3,6000]]
d=pd.DataFrame(newdata,columns=['SkinThickness','BMI'])
#d=[[15,1700]]
predictedClusters=kmeans.predict(d)
print(len(predictedClusters))
print(d)
print(predictedClusters)

"""In this week's lab We worked with the diabetes datasets for practising the unsupervised learning with K-Means clustering. We first, loaded the dataset and checked  its structure. Then we selected SkinThikness and BMI features as they were suitable for visual clusturing. Then we used the Elbow method with KElbowvisualizer just before choosing the number of clusters in order to check how the inertia changed as K increased. And from the plot, it appeared that three clusters where reasonable choice and thus we applied K-Means with k=3. After the model was fitted, We assigned the cluster lables back to the dataframe to check where the rows belonged the groups. We also checked the size and centroid of the cluster to check the average position of each cluster in terms of specific features. To check how well the model spearates the data, we calculated the silhouette scroe which shows how similar each point was as compared to the cluster it was assigned and the other clusters, which gives us a rough idea of the clustering quality. Then after, we worked using scatter graphs to see how the clusters distribute, and the clolours showes it clearly how the groups were formed based on SkinThickness and BMI, the visualisations made it very easy to understand the results of the clusturing. Then we tested the model by giving it new data points and predicting which clusters they belong to, this helped me understand how the K-Means handel new samples. Basically, this week's lab, helped me understand a lot about K-Means clustering, its working, testing and results and also helped me understand supervised and unsupeevised learning."""
